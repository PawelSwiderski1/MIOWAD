{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d3d3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255081ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(x):\n",
    "    return np.ones_like(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1.0 - np.tanh(x)**2\n",
    "\n",
    "def mse(predictions, targets):\n",
    "    return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "def cost_derivative(output_activations, y):\n",
    "    return output_activations - y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76780f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layer_sizes, activation='relu', scaler=None, verbose=False, beta_momentum=0.90, beta_rmsprop=0.999, epsilon=1e-8):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_name = activation\n",
    "        self.activation, self.activation_derivative = {\n",
    "            'linear': (linear, linear_derivative),\n",
    "            'sigmoid': (sigmoid, sigmoid_derivative),\n",
    "            'tanh': (tanh, tanh_derivative),\n",
    "            'relu': (relu, relu_derivative),\n",
    "        }[activation]\n",
    "        self.weights = [np.random.rand(y, x) for x, y in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "        self.biases = [np.random.randn(y, 1) for y in layer_sizes[1:]]\n",
    "        self.scaler = scaler\n",
    "        self.beta_momentum = beta_momentum\n",
    "        self.beta_rmsprop = beta_rmsprop\n",
    "        self.epsilon = epsilon\n",
    "        # Initialize momentum and RMSprop caches\n",
    "        self.vdw = [np.zeros_like(w) for w in self.weights]\n",
    "        self.sdw = [np.zeros_like(w) for w in self.weights]\n",
    "        self.vdb = [np.zeros_like(b) for b in self.biases]\n",
    "        self.sdb = [np.zeros_like(b) for b in self.biases]\n",
    "        self.verbose = verbose\n",
    "        if self.verbose:\n",
    "            print(\"Initial weights: \", self.weights)\n",
    "            \n",
    "    def plot_weights(self, epoch):\n",
    "        for i, w in enumerate(self.weights):\n",
    "            plt.figure(figsize=(40, 20))\n",
    "            plt.hist(w.flatten(), bins=50)\n",
    "            plt.title(f\"Layer {i + 1} Weight Distribution\")\n",
    "            plt.xlabel(\"Weight Value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "\n",
    "            filename = f\"epoch_{epoch}_layer_{i + 1}_weights_distribution.png\"\n",
    "            filepath = f\"weights_plots/multimodal-large/{filename}\"\n",
    "\n",
    "            # Save the plot to file\n",
    "            plt.savefig(filepath)\n",
    "\n",
    "            plt.clf()\n",
    "\n",
    "    def print_final_weights_and_biases(self):\n",
    "        print(\"Final Weights and Biases:\", self.weights, self.biases)\n",
    "\n",
    "        for i, w in enumerate(self.weights):\n",
    "            plt.figure(figsize=(40, 20))\n",
    "            plt.hist(w.flatten(), bins=50)\n",
    "            plt.title(f\"Layer {i + 1} Weight Distribution\")\n",
    "            plt.xlabel(\"Weight Value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "\n",
    "            filename = f\"final_layer_{i + 1}_weights_distribution.png\"\n",
    "            filepath = f\"weights_plots/multimodal-large/{filename}\"\n",
    "\n",
    "            # Save the plot to file\n",
    "            plt.savefig(filepath)\n",
    "\n",
    "            plt.clf()\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        activations = [a]\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            z = np.dot(w, a) + b\n",
    "            a = self.activation(z)\n",
    "            activations.append(a)\n",
    "        # Linear activation for the last layer\n",
    "        z = np.dot(self.weights[-1], a) + self.biases[-1]\n",
    "        a = linear(z)\n",
    "        activations.append(a)\n",
    "        return a, activations\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        final_output, activations = self.feedforward(x)\n",
    "        zs = [np.dot(w, act) + b for w, b, act in zip(self.weights, self.biases, activations[:-1])]\n",
    "        \n",
    "        # Output layer error\n",
    "        delta = cost_derivative(final_output, y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].T)\n",
    "\n",
    "        # Backpropagate the error\n",
    "        for l in range(2, len(self.layer_sizes)):\n",
    "            z = zs[-l]\n",
    "            sp = self.activation_derivative(z)\n",
    "            delta = np.dot(self.weights[-l + 1].T, delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l - 1].T)\n",
    "\n",
    "        return nabla_w, nabla_b\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, learning_rate, lambda_, n):\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_w, delta_nabla_b = self.backprop(x, y)\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "\n",
    "        # Update velocities for weights\n",
    "        self.vdw = [self.beta_momentum * v + (1 - self.beta_momentum) * nw for v, nw in zip(self.vdw, nabla_w)]\n",
    "        self.vdb = [self.beta_momentum * v + (1 - self.beta_momentum) * nb for v, nb in zip(self.vdb, nabla_b)]\n",
    "\n",
    "        # Update squared gradients for weights\n",
    "        self.sdw = [self.beta_rmsprop * s + (1 - self.beta_rmsprop) * (nw ** 2) for s, nw in zip(self.sdw, nabla_w)]\n",
    "        self.sdb = [self.beta_rmsprop * s + (1 - self.beta_rmsprop) * (nb ** 2) for s, nb in zip(self.sdb, nabla_b)]\n",
    "\n",
    "        # Correct the bias for initial iterations for both velocity and squared gradients\n",
    "        vdw_corrected = [v / (1 - self.beta_momentum ** (i + 1)) for i, v in enumerate(self.vdw)]\n",
    "        vdb_corrected = [v / (1 - self.beta_momentum ** (i + 1)) for i, v in enumerate(self.vdb)]\n",
    "        sdw_corrected = [s / (1 - self.beta_rmsprop ** (i + 1)) for i, s in enumerate(self.sdw)]\n",
    "        sdb_corrected = [s / (1 - self.beta_rmsprop ** (i + 1)) for i, s in enumerate(self.sdb)]\n",
    "\n",
    "        # Update weights and biases with L2 regularization, RMSprop and Momentum\n",
    "        self.weights = [(1 - learning_rate * (lambda_ / n)) * w - (learning_rate / len(mini_batch)) * (\n",
    "                v / (np.sqrt(s) + self.epsilon))\n",
    "                        for w, v, s in zip(self.weights, vdw_corrected, sdw_corrected)]\n",
    "        self.biases = [b - (learning_rate / len(mini_batch)) * (v / (np.sqrt(s) + self.epsilon))\n",
    "                       for b, v, s in zip(self.biases, vdb_corrected, sdb_corrected)]\n",
    "\n",
    "    def train(\n",
    "            self,\n",
    "            training_data,\n",
    "            epochs,\n",
    "            learning_rate,\n",
    "            batch_size,\n",
    "            adaptive_learning_rate=True,\n",
    "            test_data=None,\n",
    "            treshold_f1_train=-(np.inf),\n",
    "            treshold_f1_test=-(np.inf),\n",
    "            lambda_=0.0,\n",
    "            update_method=\"batch\",\n",
    "            plot_interval=None,\n",
    "    ):\n",
    "        n = len(training_data)\n",
    "        learning_rate_init = learning_rate\n",
    "\n",
    "        for j in range(epochs):\n",
    "            if j % (epochs / 100) == 0:\n",
    "                print(\"Epoch: \", j)\n",
    "\n",
    "                f1_train = self.calculate_f1_score(training_data)\n",
    "                print(f1_train)\n",
    "                if test_data:\n",
    "                    if f1_train > treshold_f1_train:\n",
    "                        f1_test = self.calculate_f1_score(test_data)\n",
    "                        print(f1_test)\n",
    "                        if f1_test > treshold_f1_test:\n",
    "                            break\n",
    "            # Plot weights at the specified interval\n",
    "            if self.verbose and plot_interval and j % plot_interval == 0:\n",
    "                self.plot_weights(epoch=j)\n",
    "\n",
    "            np.random.shuffle(training_data)\n",
    "            if update_method == \"batch\":\n",
    "                mini_batches = [\n",
    "                    training_data[k: k + batch_size] for k in range(0, n, batch_size)\n",
    "                ]\n",
    "                for mini_batch in mini_batches:\n",
    "                    self.update_mini_batch(mini_batch, learning_rate, lambda_, n)\n",
    "            elif update_method == \"epoch\":\n",
    "                self.update_mini_batch(training_data, learning_rate, lambda_, n)\n",
    "                \n",
    "            if adaptive_learning_rate:\n",
    "                # Learning rate schedule\n",
    "                #learning_rate = learning_rate_init / (1 + 0.1 * j)\n",
    "                if j % 10 == 0:\n",
    "                    learning_rate *= 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec8b5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataScaler:\n",
    "    def __init__(self, method=\"standardization\"):\n",
    "        self.method = method\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        if self.method == \"min_max\":\n",
    "            return self.fit_transform_min_max(data)\n",
    "        elif self.method == \"standardization\":\n",
    "            return self.fit_transform_standardization(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.method == \"min_max\":\n",
    "            return self.transform_min_max(data)\n",
    "        elif self.method == \"standardization\":\n",
    "            return self.transform_standardization(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        if self.method == \"min_max\":\n",
    "            return self.inverse_transform_min_max(data)\n",
    "        elif self.method == \"standardization\":\n",
    "            return self.inverse_transform_standardization(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "    def fit_transform_min_max(self, data):\n",
    "        self.min = np.min(data, axis=0)\n",
    "        self.max = np.max(data, axis=0)\n",
    "        return (data - self.min) / (self.max - self.min)\n",
    "\n",
    "    def transform_min_max(self, data):\n",
    "        return (data - self.min) / (self.max - self.min)\n",
    "\n",
    "    def inverse_transform_min_max(self, data):\n",
    "        return data * (self.max - self.min) + self.min\n",
    "\n",
    "    def fit_transform_standardization(self, data):\n",
    "        self.mean = np.mean(data, axis=0)\n",
    "        self.std = np.std(data, axis=0)\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def transform_standardization(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform_standardization(self, data):\n",
    "        return data * self.std + self.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a2e9b9",
   "metadata": {},
   "source": [
    "## WCZYTANIE DANYCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bf7cfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_square = pd.read_csv(\"mio1/regression/multimodal-large-training.csv\")\n",
    "X_train_square = df_train_square[\"x\"].values.reshape(-1, 1)\n",
    "y_train_square = df_train_square[\"y\"].values.reshape(-1, 1)\n",
    "\n",
    "df_test_square = pd.read_csv(\"mio1/regression/multimodal-large-test.csv\")\n",
    "X_test_square = df_test_square[\"x\"].values.reshape(-1, 1)\n",
    "y_test_square = df_test_square[\"y\"].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc5eec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler for X and y with the desired scaling method\n",
    "scaler_X = DataScaler(method=\"min_max\")\n",
    "scaler_y = DataScaler(method=\"min_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aa41a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler_X.fit_transform(X_train_square)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_square)\n",
    "X_test_scaled = scaler_X.fit_transform(X_test_square)\n",
    "y_test_scaled = scaler_y.fit_transform(y_test_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb193a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fe7263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_square_1_5 = MLP([1, 64, 32, 32, 1], scaler=scaler_y, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fad785b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'threshold_mse_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2g/dnyx3q9n6_3frlgv43lm7l3h0000gn/T/ipykernel_5545/2686959843.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ]\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_data_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m mlp_square_1_5.train(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtraining_data_scaled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'threshold_mse_train'"
     ]
    }
   ],
   "source": [
    "training_data_scaled = [\n",
    "    (x.reshape(-1, 1), y) for x, y in zip(X_train_scaled, y_train_scaled)\n",
    "]\n",
    "test_data_scaled = [(x.reshape(-1, 1), y) for x, y in zip(X_test_scaled, y_test_scaled)]\n",
    "mlp_square_1_5.train(\n",
    "    training_data_scaled,\n",
    "    epochs=300,\n",
    "    learning_rate=1,\n",
    "    batch_size=10,\n",
    "    test_data=test_data_scaled,\n",
    "    threshold_mse_train=20,\n",
    "    threshold_mse_test=9,\n",
    "    plot_interval=20,\n",
    ")\n",
    "\n",
    "\n",
    "# Scale the test data using the transform method (DO NOT refit scaler)\n",
    "X_test_scaled = scaler_X.transform(X_test_square)\n",
    "\n",
    "# Generate predictions on the scaled test data and inverse transform\n",
    "predictions_scaled = np.array(\n",
    "    [mlp_square_1_5.feedforward(x.reshape(-1, 1))[0] for x in X_test_scaled]\n",
    ")\n",
    "predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1))\n",
    "\n",
    "train_predictions_scaled = np.array(\n",
    "    [mlp_square_1_5.feedforward(x.reshape(-1, 1))[0] for x in X_train_scaled]\n",
    ")\n",
    "train_predictions = scaler_y.inverse_transform(train_predictions_scaled.reshape(-1, 1))\n",
    "# Calculate MSE score\n",
    "for i in range(len(predictions)):\n",
    "    print(\"predicted value: \", predictions[i], \"actual value: \", y_test_square[i])\n",
    "mse_score_train = mse(train_predictions, y_train_square)\n",
    "\n",
    "print(f\"Train MSE Score: {mse_score_train}\")\n",
    "mse_score = mse(predictions, y_test_square)\n",
    "\n",
    "print(f\"MSE Score: {mse_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e909272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
